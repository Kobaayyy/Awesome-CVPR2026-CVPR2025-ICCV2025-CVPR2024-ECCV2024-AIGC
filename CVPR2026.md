# Awesome-CVPR2026-AIGC[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A Collection of Papers and Codes for CVPR2026 AIGC

**整理汇总下今年CVPR AIGC相关的论文和代码，包括图像生成，图像编辑，视频生成，视频编辑，3D生成，3D编辑，多模态大语言模型等任务，具体如下。**

**欢迎star，fork和PR~**

**Please feel free to star, fork or PR if helpful~**

# 相关整理

- [Awesome-ECCV2024-AIGC](https://github.com/Kobaayyy/Awesome-CVPR2024-ECCV2024-AIGC/blob/main/ECCV2024.md)
- [Awesome-AIGC-Research-Groups](https://github.com/Kobaayyy/Awesome-AIGC-Research-Groups)
- [Awesome-CVPR2025/CVPR2024/CVPR2021/CVPR2020-Low-Level-Vision](https://github.com/Kobaayyy/Awesome-CVPR2025-CVPR2024-CVPR2021-CVPR2020-Low-Level-Vision)
- [Awesome-Low-Level-Vision-Research-Groups](https://github.com/Kobaayyy/Awesome-Low-Level-Vision-Research-Groups)
- [Awesome-ECCV2020-Low-Level-Vision](https://github.com/Kobaayyy/Awesome-ECCV2020-Low-Level-Vision)
  
# **参考或转载请注明出处**

CVPR2026官网：[https://cvpr.thecvf.com/Conferences/2026](https://cvpr.thecvf.com/Conferences/2026)

CVPR接收论文列表：[https://cvpr.thecvf.com/Conferences/2026/AcceptedPapers](https://cvpr.thecvf.com/Conferences/2026/AcceptedPapers)

CVPR完整论文库：

开会时间：2025月6月11日-2025月6月15日

论文接收公布时间：2025年2月27日

**【Contents】**

- [1.图像生成(Image Generation/Image Synthesis)](#1.图像生成)
- [2.图像编辑（Image Editing)](#2.图像编辑)
- [3.视频生成(Video Generation/Image Synthesis)](#3.视频生成)
- [4.视频编辑(Video Editing)](#4.视频编辑)
- [5.3D生成(3D Generation/3D Synthesis)](#5.3D生成)
- [6.3D编辑(3D Editing)](#6.3D编辑)
- [7.多模态大语言模型(Multi-Modal Large Language Model)](#7.大语言模型)
- [8.其他任务(Others)](#8.其他)

<a name="1.图像生成"></a>

# 1.图像生成(Image Generation/Image Synthesis)

### xxx
- Paper: 
- Code: 

### DDT: Decoupled Diffusion Transformer
- Paper: https://arxiv.org/abs/2504.05741
- Code: https://github.com/MCG-NJU/DDT

### Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache
- Paper: 
- Code: https://github.com/argsss/DPCache
  
### Fine-Grained GRPO for Precise Preference Alignment in Flow Models
- Paper: https://arxiv.org/abs/2510.01982
- Code: https://github.com/bcmi/Granular-GRPO

### Guiding a Diffusion Transformer with the Internal Dynamics of Itself
- Paper: https://arxiv.org/abs/2512.24176v1
- Code: https://github.com/CVL-UESTC/Internal-Guidance
  
### Guiding Diffusion-based Reconstruction with Contrastive Signals for Balanced Visual Representation
- Paper: 
- Code: https://github.com/boyuh/DCR
  
### Markovian Scale Prediction: A New Era of Visual Autoregressive Generation
- Paper: https://arxiv.org/abs/2511.23334
- Code: https://github.com/luokairo/Markov-VAR

### MultiBanana: A Challenging Benchmark for Multi-Reference Text-to-Image Generation
- Paper: https://arxiv.org/abs/2511.22989
- Code: https://github.com/matsuolab/multibanana

### PositionIC: Unified Position and Identity Consistency for Image Customization
- Paper: https://arxiv.org/abs/2507.13861
- Code: https://github.com/MeiGen-AI/PositionIC

### PosterOmni: Generalized Artistic Poster Creation via Task Distillation and Unified Reward Feedback
- Paper: https://arxiv.org/abs/2602.12127
- Code: https://github.com/MeiGen-AI/PosterOmni

### PosterReward: Unlocking Accurate Evaluation for High-Quality Graphic Design Generation
- Paper: https://alexlai2860.github.io/mypaper/posterreward/PosterReward_Arxiv_official.pdf
- Code:
    
### Semantic Alignment for Pose-Invariant Identity Preserving Diffusion
- Paper: 
- Code: https://github.com/jwonkm/SeAl
  
### Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion
- Paper: https://arxiv.org/abs/2512.04926
- Code: https://github.com/yuemingPAN/SFD

### SenCache: Accelerating Diffusion Model Inference via Sensitivity-Aware Caching
- Paper: 
- Code: https://github.com/vita-epfl/SenCache

### Training-free Mixed-Resolution Latent Upsampling for Spatially Accelerated Diffusion Transformers
- Paper: https://arxiv.org/abs/2507.08422
- Code: https://github.com/ignoww/RALU
  
### USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning
- Paper: https://arxiv.org/abs/2508.18966
- Code: https://github.com/bytedance/USO

### $\text{ViT}^3$: Unlocking Test-Time Training in Vision
- Paper: https://arxiv.org/abs/2512.01643
- Code: https://github.com/LeapLabTHU/ViTTT



  
<a name="2.图像编辑"></a>

# 2.图像编辑(Image Editing)

### ChordEdit: One-Step Low-Energy Transport for Image Editing
- Paper: https://arxiv.org/abs/2602.19083
- Code: https://github.com/ChordEdit/ChordEdit

### FEAT: Fashion Editing and Try-On from Any Design
- Paper: 
- Code: https://github.com/soyekwon/FEAT


  
<a name="3.视频生成"></a>

# 3.视频生成(Video Generation/Video Synthesis)

### HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives
- Paper: https://arxiv.org/abs/2510.20822
- Code: https://github.com/yihao-meng/HoloCine

### Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout
- Paper: https://arxiv.org/abs/2511.20649
- Code: https://github.com/yesiltepe-hidir/infinity-rope
  
### MultiShotMaster: A Controllable Multi-Shot Video Generation Framework
- Paper: https://arxiv.org/abs/2512.03041
- Code: https://github.com/KlingAIResearch/MultiShotMaster
  
### Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation
- Paper: https://arxiv.org/abs/2512.04678
- Code: https://github.com/JaydenLyh/Reward-Forcing

### Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation
- Paper: https://arxiv.org/abs/2508.07901
- Code: https://github.com/WeChatCV/Stand-In


  
<a name="4.视频编辑"></a>

# 4.视频编辑(Video Editing)

### Eevee: Towards Close-up High-resolution Video-based Virtual Try-on
- Paper: https://arxiv.org/abs/2511.18957
- Code: https://github.com/AMAP-ML/Eevee
  
### FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction
- Paper: https://arxiv.org/abs/2512.16900
- Code: https://github.com/Francis-Rings/FlashPortrait
  
### PersonaLive! Expressive Portrait Image Animation for Live Streaming
- Paper: https://arxiv.org/abs/2512.11253
- Code: https://github.com/GVCLab/PersonaLive


<a name="5.3D生成"></a>

# 5.3D生成(3D Generation/3D Synthesis)

### Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation
- Paper: https://arxiv.org/abs/2512.10949
- Code: https://github.com/Ivan-Tang-3D/3DGen-R1

### PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image
- Paper: https://arxiv.org/abs/2511.13648
- Code: https://github.com/ziangcao0312/PhysX-Anything
  
<a name="6.3D编辑"></a>

# 6.3D编辑(3D Editing)


  
<a name="7.大语言模型"></a>

# 7.多模态大语言模型(Multi-Modal Large Language Models)

### LongVT: Incentivizing "Thinking with Long Videos" via Native Tool Calling
- Paper: https://arxiv.org/abs/2511.20785
- Code: https://github.com/EvolvingLMMs-Lab/LongVT

### OneThinker: All-in-one Reasoning Model for Image and Video
- Paper: https://arxiv.org/abs/2512.03043
- Code: https://github.com/tulerfeng/OneThinker


  
<a name="8.其他"></a>

# 8.其他任务(Others)

### Fine-grained Image Aesthetic Assessment: Learning Discriminative Scores from Relative Ranks
- Paper: 
- Code: https://github.com/yzc-ippl/FG-IAA

### Relational Visual Similarity
- Paper: https://arxiv.org/abs/2512.07833
- Code: https://github.com/thaoshibe/relsim
  
<font color=red size=5>持续更新~</font>
